{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e92bb1fa6841f2840a9400027c8bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 23:24:20 INFO: Downloading these customized packages for language: ru (Russian)...\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| pretrain  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2021-10-02 23:24:20 INFO: File exists: C:\\Users\\Acer\\stanza_resources\\ru\\tokenize\\syntagrus.pt.\n",
      "2021-10-02 23:24:20 INFO: File exists: C:\\Users\\Acer\\stanza_resources\\ru\\pos\\syntagrus.pt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 23:24:21 INFO: File exists: C:\\Users\\Acer\\stanza_resources\\ru\\pretrain\\syntagrus.pt.\n",
      "2021-10-02 23:24:21 INFO: Finished downloading models and saved to C:\\Users\\Acer\\stanza_resources.\n",
      "2021-10-02 23:24:21 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "=========================\n",
      "\n",
      "2021-10-02 23:24:21 INFO: Use device: cpu\n",
      "2021-10-02 23:24:21 INFO: Loading: tokenize\n",
      "2021-10-02 23:24:21 INFO: Loading: pos\n",
      "2021-10-02 23:24:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "import stanza\n",
    "stanza.download('ru', processors='tokenize,pos')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,pos')\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала прогоняем текст через Pymorphy и Stanza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "with open ('text.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "    py_tuples = []\n",
    "    pr_word = ''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in '.-?,!:;—»«':\n",
    "            p = m.parse(token)\n",
    "            pos = p[0].tag.POS\n",
    "            if p[0].word != pr_word:\n",
    "                t = (p[0].word, pos)\n",
    "                py_tuples.append(t)\n",
    "                pr_word = p[0].word\n",
    "    st_tuples = []\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            t = (word.text, word.pos)\n",
    "            st_tuples.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем через Mystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stem_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7d7712fd452e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'analysis'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mstem_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stem_tags' is not defined"
     ]
    }
   ],
   "source": [
    "with open ('text.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "    from pymystem3 import Mystem\n",
    "    m = Mystem()\n",
    "    ana = m.analyze(text)\n",
    "stem_tuples = []\n",
    "for word in ana:\n",
    "    if 'analysis' in word:\n",
    "        if word['analysis']:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            stem_tuples.append((word['text'], pos))\n",
    "        else:\n",
    "            stem_tuples.append((word['text'], ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем списки с тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "right_tags = []\n",
    "with open('corpus.csv', encoding = 'utf-8') as File:\n",
    "    reader = csv.DictReader(File, delimiter = ',')\n",
    "    for row in reader:\n",
    "        right_tags.append(row['tag'])\n",
    "stanza_tags = []\n",
    "for word in st_tuples:\n",
    "    if word[1] != 'PUNCT':\n",
    "        stanza_tags.append(word[1])\n",
    "py_tags = []\n",
    "for word in py_tuples:\n",
    "    py_tags.append(word[1])\n",
    "stem_tags = []\n",
    "for word in stem_tuples:\n",
    "    if word[0] == 'А':\n",
    "        stem_tags.append('N')\n",
    "    if word[0] == 'бе' or word[0] == 'ве' or word[0] == 'ге' or word[0] == 'де' or word[0] == 'е' or word[0] == 'же' or word[0] == 'зе' or word[0] == 'кома':\n",
    "        continue\n",
    "    stem_tags.append(word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приводим все к единому тагсету и считаем accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7929515418502202\n",
      "0.8766519823788547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "new_stem_tags = []\n",
    "nnew_stanza_tags = []\n",
    "for tag in stem_tags:\n",
    "    if tag == 'S':\n",
    "        new_stem_tags.append('NOUN')\n",
    "    elif tag == 'V':\n",
    "        new_stem_tags.append('VERB')\n",
    "    elif tag == 'A':\n",
    "        new_stem_tags.append('ADJ')\n",
    "    elif tag == 'PR':\n",
    "        new_stem_tags.append('PREP')\n",
    "    elif tag == 'APRO':\n",
    "        new_stem_tags.append('ADJF')\n",
    "    elif tag == 'SPRO':\n",
    "        new_stem_tags.append('NPRO')\n",
    "    else:\n",
    "        new_stem_tags.append(tag)\n",
    "for tag in stanza_tags:\n",
    "    if tag == 'ADP':\n",
    "        nnew_stanza_tags.append('PREP')\n",
    "    elif tag == 'CCONJ':\n",
    "        nnew_stanza_tags.append('CONJ')\n",
    "    elif tag == 'DET':\n",
    "        nnew_stanza_tags.append('ADJF')\n",
    "    else:\n",
    "        nnew_stanza_tags.append(tag)\n",
    "new_py_tags = []\n",
    "for tag in py_tags:\n",
    "    if tag == 'ADJF' or tag == 'ADJS':\n",
    "        new_py_tags.append('ADJ')\n",
    "    elif tag == None:\n",
    "        new_py_tags.append('')\n",
    "    elif tag == 'PRCL':\n",
    "        new_py_tags.append('PART')\n",
    "    elif tag == 'ADVB':\n",
    "        new_py_tags.append('ADV')\n",
    "    elif tag == 'GRND' or tag == 'PRTF' or tag == 'INFN':\n",
    "        new_py_tags.append('ADV')\n",
    "    else:\n",
    "        new_py_tags.append(tag)\n",
    "new_right_tags = []\n",
    "for tag in right_tags:\n",
    "    if tag == 'ADJF' or tag == 'ADJS':\n",
    "        new_right_tags.append('ADJ')\n",
    "    elif tag == 'PRCL':\n",
    "        new_right_tags.append('PART')\n",
    "    elif tag == 'ADVB':\n",
    "        new_right_tags.append('ADV')\n",
    "    elif tag == 'GRND' or tag == 'PRTF' or tag == 'INFN':\n",
    "        new_right_tags.append('ADV')\n",
    "    else:\n",
    "        new_right_tags.append(tag)\n",
    "stem_accuracy = accuracy_score(new_stem_tags, new_right_tags)\n",
    "py_accuracy = accuracy_score(new_py_tags, new_right_tags)\n",
    "stanza_accuracy = accuracy_score(nnew_stanza_tags, new_right_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ищем н-граммы с помощью Pymorphy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['на', 'верхний', 'этаж'], ['от', 'страшной', 'картины'], ['в', 'крученыховском', 'аде']]\n"
     ]
    }
   ],
   "source": [
    "prep_adj_nouns = []\n",
    "prep_adj_noun = []\n",
    "adv_verbs = []\n",
    "adv_verb = []\n",
    "verb_noun = []\n",
    "verb_nouns = []\n",
    "for ind in range(len(py_tags)):\n",
    "    if ind == len(py_tags)-1:\n",
    "        pass\n",
    "    elif py_tuples[ind][1] == 'PREP' and py_tuples[ind+1][1] == 'ADJF' and py_tuples[ind+2][1] == 'NOUN':\n",
    "        prep_adj_noun.append(py_tuples[ind][0])\n",
    "        prep_adj_noun.append(py_tuples[ind+1][0])\n",
    "        prep_adj_noun.append(py_tuples[ind+2][0])\n",
    "        prep_adj_nouns.append(prep_adj_noun)\n",
    "        prep_adj_noun = []\n",
    "    if ind == len(py_tags)-1:\n",
    "        pass\n",
    "    elif py_tuples[ind][1] == 'ADVB' and py_tuples[ind+1][1] == 'VERB':\n",
    "        adv_verb.append(py_tuples[ind][0])\n",
    "        adv_verb.append(py_tuples[ind+1][0])\n",
    "        adv_verbs.append(adv_verb)\n",
    "        adv_verb = []\n",
    "    elif py_tuples[ind][0] == 'не' and py_tuples[ind+1][1] == 'VERB':\n",
    "        verb_noun.append(py_tuples[ind][0])\n",
    "        verb_noun.append(py_tuples[ind+1][0])\n",
    "        verb_nouns.append(verb_noun)\n",
    "        verb_noun = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем задании можно использовать н-грамму не+прилаг/наречие (для негативных отзывов) и н-грамму очень+прилаг (для положительных), что я и попыталась реализовать в предыдущем коде"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
